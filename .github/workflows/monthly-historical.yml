name: Monthly Historical Data Processing (02:07)

on:
  schedule:
    # UTC æ—¶é—´ 18:07 æ¯æœˆç¬¬å››ä¸ªå‘¨å›› = åŒ—äº¬æ—¶é—´ 02:07 æ¯æœˆç¬¬å››ä¸ªå‘¨äº” (é¿å…å†²çª)
    - cron: '7 18 22-28 * 4'
  workflow_dispatch:

permissions:
  contents: write
  actions: read

jobs:
  monthly-historical-processing:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
      
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Install Playwright browsers
      run: |
        playwright install chromium
        
    - name: Configure git
      run: |
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        
    - name: Debug environment and configuration
      env:
        BILIBILI_COOKIES: ${{ secrets.BILIBILI_COOKIES }}
        BILIBILI_COOKIES_1: ${{ secrets.BILIBILI_COOKIES_1 }}
        BILIBILI_COOKIES_2: ${{ secrets.BILIBILI_COOKIES_2 }}
        BILIBILI_COOKIES_3: ${{ secrets.BILIBILI_COOKIES_3 }}
        BILIBILI_COOKIES_4: ${{ secrets.BILIBILI_COOKIES_4 }}
        BILIBILI_COOKIES_5: ${{ secrets.BILIBILI_COOKIES_5 }}
        BILIBILI_COOKIES_6: ${{ secrets.BILIBILI_COOKIES_6 }}
        BILIBILI_COOKIES_7: ${{ secrets.BILIBILI_COOKIES_7 }}
        BILIBILI_COOKIES_8: ${{ secrets.BILIBILI_COOKIES_8 }}
        BILIBILI_COOKIES_9: ${{ secrets.BILIBILI_COOKIES_9 }}
        BILIBILI_COOKIES_10: ${{ secrets.BILIBILI_COOKIES_10 }}
      run: |
        echo "ðŸ” Running environment debug check"
        python .github/scripts/debug_data_collection.py
        
    - name: Enhanced cookie rotation and selection
      env:
        UP_ID: ${{ secrets.UP_ID }}
        BILIBILI_COOKIES: ${{ secrets.BILIBILI_COOKIES }}
        BILIBILI_COOKIES_1: ${{ secrets.BILIBILI_COOKIES_1 }}
        BILIBILI_COOKIES_2: ${{ secrets.BILIBILI_COOKIES_2 }}
        BILIBILI_COOKIES_3: ${{ secrets.BILIBILI_COOKIES_3 }}
        BILIBILI_COOKIES_4: ${{ secrets.BILIBILI_COOKIES_4 }}
        BILIBILI_COOKIES_5: ${{ secrets.BILIBILI_COOKIES_5 }}
        BILIBILI_COOKIES_6: ${{ secrets.BILIBILI_COOKIES_6 }}
        BILIBILI_COOKIES_7: ${{ secrets.BILIBILI_COOKIES_7 }}
        BILIBILI_COOKIES_8: ${{ secrets.BILIBILI_COOKIES_8 }}
        BILIBILI_COOKIES_9: ${{ secrets.BILIBILI_COOKIES_9 }}
        BILIBILI_COOKIES_10: ${{ secrets.BILIBILI_COOKIES_10 }}
      run: |
        echo "ðŸ”„ Starting enhanced cookie rotation with fallback mechanism"
        python .github/scripts/enhanced_cookie_rotation.py
        
    - name: Cookie cleanup check
      if: always()
      run: |
        echo "ðŸ—‘ï¸ Checking for failed cookies cleanup"
        python .github/scripts/cookie_cleanup_manager.py || echo "Cleanup check completed with warnings"
        
    - name: Run monthly historical data processing with enhanced notification
      env:
        UP_ID: ${{ secrets.UP_ID }}
        FEISHU_WEBHOOK_URL: ${{ secrets.FEISHU_WEBHOOK_URL }}
        PYTHONPATH: ${{ github.workspace }}
      run: |
        echo "Starting weekly historical data processing with enhanced notification"
        echo "Current working directory: $(pwd)"
        echo "Python path: $PYTHONPATH"
        echo "Contents of current directory:"
        ls -la
        
        # å‘é€å¼€å§‹é€šçŸ¥
        if [ -n "$FEISHU_WEBHOOK_URL" ]; then
          echo "ðŸ“¤ å‘é€ä»»åŠ¡å¼€å§‹é€šçŸ¥..."
          curl -X POST -H "Content-Type: application/json" -d '{
            "msg_type": "interactive",
            "card": {
              "elements": [
                {
                  "tag": "div",
                  "text": {
                    "content": "**ðŸ“¦ ä»“åº“**: Bilibili_Flow\n**ðŸ”„ å·¥ä½œæµ**: Monthly Historical Data Processing\n**ðŸ“… æ—¶é—´**: '$(date '+%Y-%m-%d %H:%M:%S')'\n**ðŸ·ï¸ è¿è¡Œ**: #${{ github.run_number }}\n**ðŸŒ¿ åˆ†æ”¯**: ${{ github.ref_name }}\n**ðŸš€ çŠ¶æ€**: å¼€å§‹æ‰§è¡Œæœˆåº¦åŽ†å²æ•°æ®å¤„ç†",
                    "tag": "lark_md"
                  }
                }
              ],
              "header": {
                "title": {
                  "content": "ðŸš€ æœˆåº¦æ•°æ®å¤„ç†å¼€å§‹",
                  "tag": "plain_text"
                },
                "template": "blue"
              }
            }
          }' "$FEISHU_WEBHOOK_URL" || echo "é£žä¹¦é€šçŸ¥å‘é€å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œä»»åŠ¡"
        fi
        
        # è¿è¡Œæœˆä»»åŠ¡å¹¶è®°å½•ç»“æžœ
        TASK_START_TIME=$(date +%s)
        if ! python run_monthly_task.py; then
          TASK_END_TIME=$(date +%s)
          TASK_DURATION=$((TASK_END_TIME - TASK_START_TIME))
          echo "âŒ Monthly task execution failed"
          echo "Checking for any generated files:"
          find . -name "*.json" -o -name "*.db" | head -10
          
          # å‘é€å¤±è´¥é€šçŸ¥
          if [ -n "$FEISHU_WEBHOOK_URL" ]; then
            echo "ðŸ“¤ å‘é€ä»»åŠ¡å¤±è´¥é€šçŸ¥..."
            curl -X POST -H "Content-Type: application/json" -d '{
              "msg_type": "interactive",
              "card": {
                "elements": [
                  {
                    "tag": "div",
                    "text": {
                      "content": "**ðŸ“¦ ä»“åº“**: Bilibili_Flow\n**ðŸ”„ å·¥ä½œæµ**: Monthly Historical Data Processing\n**ðŸ“… æ—¶é—´**: '$(date '+%Y-%m-%d %H:%M:%S')'\n**ðŸ·ï¸ è¿è¡Œ**: #${{ github.run_number }}\n**ðŸŒ¿ åˆ†æ”¯**: ${{ github.ref_name }}\n**â±ï¸ æ‰§è¡Œæ—¶é•¿**: '$(($TASK_DURATION / 60))' åˆ†é’Ÿ\n**âŒ çŠ¶æ€**: ä»»åŠ¡æ‰§è¡Œå¤±è´¥\n\n**é”™è¯¯ä¿¡æ¯**: è¯·æŸ¥çœ‹å·¥ä½œæµæ—¥å¿—èŽ·å–è¯¦ç»†ä¿¡æ¯",
                      "tag": "lark_md"
                    }
                  }
                ],
                "header": {
                  "title": {
                    "content": "âŒ æœˆåº¦æ•°æ®å¤„ç†å¤±è´¥",
                    "tag": "plain_text"
                  },
                  "template": "red"
                }
              }
            }' "$FEISHU_WEBHOOK_URL" || echo "é£žä¹¦é€šçŸ¥å‘é€å¤±è´¥"
          fi
          exit 1
        else
          TASK_END_TIME=$(date +%s)
          TASK_DURATION=$((TASK_END_TIME - TASK_START_TIME))
          echo "âœ… Monthly task execution completed"
          echo "Checking generated data files:"
          find data/ -type f | head -10
          
          # ç»Ÿè®¡ç”Ÿæˆçš„æ–‡ä»¶
          JSON_COUNT=$(find data/ -name "*.json" | wc -l)
          DB_COUNT=$(find data/ -name "*.db" | wc -l)
          TOTAL_SIZE=$(du -sh data/ 2>/dev/null | cut -f1 || echo "æœªçŸ¥")
          
          # å‘é€æˆåŠŸé€šçŸ¥
          if [ -n "$FEISHU_WEBHOOK_URL" ]; then
            echo "ðŸ“¤ å‘é€ä»»åŠ¡æˆåŠŸé€šçŸ¥..."
            curl -X POST -H "Content-Type: application/json" -d '{
              "msg_type": "interactive",
              "card": {
                "elements": [
                  {
                    "tag": "div",
                    "text": {
                      "content": "**ðŸ“¦ ä»“åº“**: Bilibili_Flow\n**ðŸ”„ å·¥ä½œæµ**: Monthly Historical Data Processing\n**ðŸ“… æ—¶é—´**: '$(date '+%Y-%m-%d %H:%M:%S')'\n**ðŸ·ï¸ è¿è¡Œ**: #${{ github.run_number }}\n**ðŸŒ¿ åˆ†æ”¯**: ${{ github.ref_name }}\n**â±ï¸ æ‰§è¡Œæ—¶é•¿**: '$(($TASK_DURATION / 60))' åˆ†é’Ÿ\n**âœ… çŠ¶æ€**: ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ\n\n**ðŸ“Š æ•°æ®ç»Ÿè®¡**\nðŸ“„ JSONæ–‡ä»¶: '$JSON_COUNT' ä¸ª\nðŸ—„ï¸ æ•°æ®åº“æ–‡ä»¶: '$DB_COUNT' ä¸ª\nðŸ’¾ æ•°æ®æ€»å¤§å°: '$TOTAL_SIZE'\n\n**ðŸŽ¯ ä»»åŠ¡å®Œæˆ**: æœˆåº¦åŽ†å²æ•°æ®å¤„ç†å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜åˆ°ä»“åº“",
                      "tag": "lark_md"
                    }
                  }
                ],
                "header": {
                  "title": {
                    "content": "âœ… æœˆåº¦æ•°æ®å¤„ç†æˆåŠŸ",
                    "tag": "plain_text"
                  },
                  "template": "green"
                }
              }
            }' "$FEISHU_WEBHOOK_URL" || echo "é£žä¹¦é€šçŸ¥å‘é€å¤±è´¥"
          fi
        fi
        
    - name: Commit and push data
      run: |
        echo "ðŸ” Checking for changes in data directory"
        
        # Force add all data subdirectories to ensure new files are tracked
        if [ -d "data/" ]; then
          git add -A data/
          
          # Check if there are changes to commit
          if ! git diff --staged --quiet; then
            git commit -m "ðŸ“Š Weekly historical data processing - $(date +'%Y-%m-%d %H:%M:%S')"
            git push
            echo "âœ… Data committed and pushed successfully"
          else
            echo "â„¹ï¸ No changes to commit in data directory"
          fi
        else
          echo "âš ï¸ Data directory doesn't exist - no data to commit"
        fi
        
    - name: Generate data summary report
      if: always()
      run: |
        echo "## Weekly Data Processing Report - $(date +'%Y-%m-%d')" > weekly_report.md
        echo "### Processing Summary" >> weekly_report.md
        
        # Check if data directory exists and has content
        if [ -d "data/" ] && [ "$(find data/ -name "*.json" | wc -l)" -gt 0 ]; then
          echo "- âœ… Data directory found with JSON files" >> weekly_report.md
          echo "- âœ… Historical data processing appears successful" >> weekly_report.md
          echo "- âœ… 28-day segmentation applied" >> weekly_report.md
          echo "- âœ… Database baseline established" >> weekly_report.md
        else
          echo "- âŒ Historical data processing encountered errors" >> weekly_report.md
          echo "- âš ï¸ Data directory empty or missing JSON files" >> weekly_report.md
          echo "- âš ï¸ Check workflow logs for details" >> weekly_report.md
        fi
        
        echo "" >> weekly_report.md
        echo "### File Structure" >> weekly_report.md
        if [ -d "data/" ]; then
          echo "Data directory contents:" >> weekly_report.md
          find data/ -type f -name "*.json" | head -20 | while read file; do
            size=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null || echo "unknown")
            echo "- $file (${size} bytes)" >> weekly_report.md
          done
          
          if [ "$(find data/ -name "*.json" | wc -l)" -eq 0 ]; then
            echo "- No JSON files found in data directory" >> weekly_report.md
          fi
        else
          echo "- Data directory not found" >> weekly_report.md
        fi
        
        echo "" >> weekly_report.md
        echo "### System Information" >> weekly_report.md
        echo "- Workflow: weekly-historical-processing" >> weekly_report.md
        echo "- Runner: ubuntu-latest" >> weekly_report.md
        echo "- Python version: 3.11" >> weekly_report.md
        echo "- Timestamp: $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> weekly_report.md
        
        # Create placeholder files if they don't exist
        touch cookie_management_report.json
        touch cookie_cleanup_report.json  
        touch failed_cookies_report.json
        touch cleanup_cookies.sh
        
        # Add basic content to empty files with timestamps
        timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
        [ ! -s cookie_management_report.json ] && echo "{\"status\": \"no_report_generated\", \"timestamp\": \"$timestamp\", \"workflow\": \"weekly-historical\"}" > cookie_management_report.json
        [ ! -s cookie_cleanup_report.json ] && echo "{\"status\": \"no_cleanup_performed\", \"timestamp\": \"$timestamp\", \"workflow\": \"weekly-historical\"}" > cookie_cleanup_report.json
        [ ! -s failed_cookies_report.json ] && echo "{\"failed_cookies\": [], \"timestamp\": \"$timestamp\", \"workflow\": \"weekly-historical\"}" > failed_cookies_report.json
        [ ! -s cleanup_cookies.sh ] && echo "#!/bin/bash\necho \"No cleanup script generated for weekly-historical workflow - $timestamp\"" > cleanup_cookies.sh
        
    - name: Upload report and cookie reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: weekly-report-${{ github.run_number }}
        path: |
          weekly_report.md
          debug_report.json
          cookie_management_report.json
          cookie_cleanup_report.json
          failed_cookies_report.json
          cleanup_cookies.sh
        retention-days: 30
