name: Monthly Historical Data Processing

"on":
  workflow_dispatch:

permissions:
  contents: write
  actions: read

jobs:
  monthly-historical-processing:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
        ref: sync-current-files  # Explicitly checkout the latest sync-current-files branch
      
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Cache data files
      uses: actions/cache@v4
      with:
        path: |
          data/cookies.json
          data/backup_cookies
        key: bilibili-data-${{ runner.os }}-${{ hashFiles('data/cookies.json') }}-v1
        restore-keys: |
          bilibili-data-${{ runner.os }}-v1
          bilibili-data-${{ runner.os }}-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Install Playwright browsers
      run: |
        playwright install chromium
        
    - name: Configure git for data conflicts
      run: |
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        # Configure git to handle database file conflicts
        git config merge.ours.driver true
        echo "data/database/bilibili_tracking.db merge=ours" >> .gitattributes
        git add .gitattributes || true
        
        # Ensure we're on the latest version before starting tasks
        echo "🔄 Ensuring latest codebase..."
        git fetch origin sync-current-files
        git reset --hard origin/sync-current-files
        
    - name: Run monthly historical data processing with notification
      env:
        UP_ID: ${{ secrets.UP_ID }}
        FEISHU_WEBHOOK_URL: ${{ secrets.FEISHU_WEBHOOK_URL }}
        PYTHONPATH: ${{ github.workspace }}
        GITHUB_RUN_NUMBER: ${{ github.run_number }}
        GITHUB_REF_NAME: ${{ github.ref_name }}
        GITHUB_REPOSITORY: ${{ github.repository }}
        # Core组件需要的Cookie环境变量
        BILIBILI_COOKIES: ${{ secrets.BILIBILI_COOKIES }}
        BILIBILI_COOKIES_1: ${{ secrets.BILIBILI_COOKIES_1 }}
        BILIBILI_COOKIES_2: ${{ secrets.BILIBILI_COOKIES_2 }}
        BILIBILI_COOKIES_3: ${{ secrets.BILIBILI_COOKIES_3 }}
        BILIBILI_COOKIES_4: ${{ secrets.BILIBILI_COOKIES_4 }}
        BILIBILI_COOKIES_5: ${{ secrets.BILIBILI_COOKIES_5 }}
        BILIBILI_COOKIES_6: ${{ secrets.BILIBILI_COOKIES_6 }}
        BILIBILI_COOKIES_7: ${{ secrets.BILIBILI_COOKIES_7 }}
        BILIBILI_COOKIES_8: ${{ secrets.BILIBILI_COOKIES_8 }}
        BILIBILI_COOKIES_9: ${{ secrets.BILIBILI_COOKIES_9 }}
        BILIBILI_COOKIES_10: ${{ secrets.BILIBILI_COOKIES_10 }}
      run: |
        echo "Starting monthly historical data processing"
        
        # Run monthly task and capture all output
        echo "Running monthly task..."
        TASK_OUTPUT_FILE="monthly_task_output_$(date +%s).log"
        
        if python run_monthly_task.py > "$TASK_OUTPUT_FILE" 2>&1; then
          echo "✅ Monthly task completed successfully"
          
          # Send success notification with extracted data
          echo "Sending success notification..."
          python .github/scripts/feishu_notifier.py "Monthly Historical Data Processing" "success" < "$TASK_OUTPUT_FILE" || echo "⚠️ Notification failed but task succeeded"
          
        else
          echo "❌ Monthly task failed"
          
          # Send failure notification with extracted data
          echo "Sending failure notification..."
          python .github/scripts/feishu_notifier.py "Monthly Historical Data Processing" "failure" < "$TASK_OUTPUT_FILE" || echo "⚠️ Notification failed and task failed"
          
          # Display output for debugging
          echo "=== Task Output ==="
          cat "$TASK_OUTPUT_FILE"
          echo "=================="
          
          exit 1
        fi
        
        # Display output for debugging (success case)
        echo "=== Task Output ==="
        cat "$TASK_OUTPUT_FILE"
        echo "=================="
        
        # Cleanup
        rm -f "$TASK_OUTPUT_FILE"
        
    - name: Commit and push data
      run: |
        echo "🔍 Checking for changes in data directory"
        
        # Force add all data subdirectories to ensure new files are tracked
        if [ -d "data/" ]; then
          git add -A data/
          
          # Check if there are changes to commit
          if ! git diff --staged --quiet; then
            git commit -m "📊 Weekly historical data processing - $(date +'%Y-%m-%d %H:%M:%S')"
            git push
            echo "✅ Data committed and pushed successfully"
          else
            echo "ℹ️ No changes to commit in data directory"
          fi
        else
          echo "⚠️ Data directory doesn't exist - no data to commit"
        fi
        
    - name: Generate data summary report
      if: always()
      run: |
        echo "## Weekly Data Processing Report - $(date +'%Y-%m-%d')" > weekly_report.md
        echo "### Processing Summary" >> weekly_report.md
        
        # Check if data directory exists and has content
        if [ -d "data/" ] && [ "$(find data/ -name "*.json" | wc -l)" -gt 0 ]; then
          echo "- ✅ Data directory found with JSON files" >> weekly_report.md
          echo "- ✅ Historical data processing appears successful" >> weekly_report.md
          echo "- ✅ 28-day segmentation applied" >> weekly_report.md
          echo "- ✅ Database baseline established" >> weekly_report.md
        else
          echo "- ❌ Historical data processing encountered errors" >> weekly_report.md
          echo "- ⚠️ Data directory empty or missing JSON files" >> weekly_report.md
          echo "- ⚠️ Check workflow logs for details" >> weekly_report.md
        fi
        
        echo "" >> weekly_report.md
        echo "### File Structure" >> weekly_report.md
        if [ -d "data/" ]; then
          echo "Data directory contents:" >> weekly_report.md
          find data/ -type f -name "*.json" | head -20 | while read file; do
            size=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null || echo "unknown")
            echo "- $file (${size} bytes)" >> weekly_report.md
          done
          
          if [ "$(find data/ -name "*.json" | wc -l)" -eq 0 ]; then
            echo "- No JSON files found in data directory" >> weekly_report.md
          fi
        else
          echo "- Data directory not found" >> weekly_report.md
        fi
        
        echo "" >> weekly_report.md
        echo "### System Information" >> weekly_report.md
        echo "- Workflow: weekly-historical-processing" >> weekly_report.md
        echo "- Runner: ubuntu-latest" >> weekly_report.md
        echo "- Python version: 3.11" >> weekly_report.md
        echo "- Timestamp: $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> weekly_report.md
        
        # Create placeholder files if they don't exist
        touch debug_report.json
        
        # Add basic content to empty files with timestamps
        timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
        [ ! -s debug_report.json ] && echo "{\"status\": \"no_debug_report_generated\", \"timestamp\": \"$timestamp\", \"workflow\": \"monthly-historical\"}" > debug_report.json
        
    - name: Upload monthly report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: monthly-report-${{ github.run_number }}
        path: |
          weekly_report.md
          debug_report.json
        retention-days: 90
