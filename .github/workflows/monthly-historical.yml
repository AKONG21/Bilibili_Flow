name: Monthly Historical Data Processing (02:07)

on:
  schedule:
    # UTC 时间 18:07 每月第四个周四 = 北京时间 02:07 每月第四个周五 (避免冲突)
    - cron: '7 18 22-28 * 4'
  workflow_dispatch:

permissions:
  contents: write
  actions: read

jobs:
  monthly-historical-processing:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
      
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Install Playwright browsers
      run: |
        playwright install chromium
        
    - name: Configure git
      run: |
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        
    - name: Debug environment and configuration
      env:
        BILIBILI_COOKIES: ${{ secrets.BILIBILI_COOKIES }}
        BILIBILI_COOKIES_1: ${{ secrets.BILIBILI_COOKIES_1 }}
        BILIBILI_COOKIES_2: ${{ secrets.BILIBILI_COOKIES_2 }}
        BILIBILI_COOKIES_3: ${{ secrets.BILIBILI_COOKIES_3 }}
        BILIBILI_COOKIES_4: ${{ secrets.BILIBILI_COOKIES_4 }}
        BILIBILI_COOKIES_5: ${{ secrets.BILIBILI_COOKIES_5 }}
        BILIBILI_COOKIES_6: ${{ secrets.BILIBILI_COOKIES_6 }}
        BILIBILI_COOKIES_7: ${{ secrets.BILIBILI_COOKIES_7 }}
        BILIBILI_COOKIES_8: ${{ secrets.BILIBILI_COOKIES_8 }}
        BILIBILI_COOKIES_9: ${{ secrets.BILIBILI_COOKIES_9 }}
        BILIBILI_COOKIES_10: ${{ secrets.BILIBILI_COOKIES_10 }}
      run: |
        echo "🔍 Running environment debug check"
        python .github/scripts/debug_data_collection.py
        
    - name: Enhanced cookie rotation and selection
      env:
        UP_ID: ${{ secrets.UP_ID }}
        BILIBILI_COOKIES: ${{ secrets.BILIBILI_COOKIES }}
        BILIBILI_COOKIES_1: ${{ secrets.BILIBILI_COOKIES_1 }}
        BILIBILI_COOKIES_2: ${{ secrets.BILIBILI_COOKIES_2 }}
        BILIBILI_COOKIES_3: ${{ secrets.BILIBILI_COOKIES_3 }}
        BILIBILI_COOKIES_4: ${{ secrets.BILIBILI_COOKIES_4 }}
        BILIBILI_COOKIES_5: ${{ secrets.BILIBILI_COOKIES_5 }}
        BILIBILI_COOKIES_6: ${{ secrets.BILIBILI_COOKIES_6 }}
        BILIBILI_COOKIES_7: ${{ secrets.BILIBILI_COOKIES_7 }}
        BILIBILI_COOKIES_8: ${{ secrets.BILIBILI_COOKIES_8 }}
        BILIBILI_COOKIES_9: ${{ secrets.BILIBILI_COOKIES_9 }}
        BILIBILI_COOKIES_10: ${{ secrets.BILIBILI_COOKIES_10 }}
      run: |
        echo "🔄 Starting enhanced cookie rotation with fallback mechanism"
        python .github/scripts/enhanced_cookie_rotation.py
        
    - name: Cookie cleanup check
      if: always()
      run: |
        echo "🗑️ Checking for failed cookies cleanup"
        python .github/scripts/cookie_cleanup_manager.py || echo "Cleanup check completed with warnings"
        
    - name: Run monthly historical data processing with enhanced notification
      env:
        UP_ID: ${{ secrets.UP_ID }}
        FEISHU_WEBHOOK_URL: ${{ secrets.FEISHU_WEBHOOK_URL }}
        PYTHONPATH: ${{ github.workspace }}
      run: |
        echo "Starting weekly historical data processing with enhanced notification"
        echo "Current working directory: $(pwd)"
        echo "Python path: $PYTHONPATH"
        echo "Contents of current directory:"
        ls -la
        
        # 发送开始通知
        if [ -n "$FEISHU_WEBHOOK_URL" ]; then
          echo "📤 发送任务开始通知..."
          curl -X POST -H "Content-Type: application/json" -d '{
            "msg_type": "interactive",
            "card": {
              "elements": [
                {
                  "tag": "div",
                  "text": {
                    "content": "**📦 仓库**: Bilibili_Flow\n**🔄 工作流**: Monthly Historical Data Processing\n**📅 时间**: '$(date '+%Y-%m-%d %H:%M:%S')'\n**🏷️ 运行**: #${{ github.run_number }}\n**🌿 分支**: ${{ github.ref_name }}\n**🚀 状态**: 开始执行月度历史数据处理",
                    "tag": "lark_md"
                  }
                }
              ],
              "header": {
                "title": {
                  "content": "🚀 月度数据处理开始",
                  "tag": "plain_text"
                },
                "template": "blue"
              }
            }
          }' "$FEISHU_WEBHOOK_URL" || echo "飞书通知发送失败，继续执行任务"
        fi
        
        # 运行月任务并记录结果
        TASK_START_TIME=$(date +%s)
        if ! python run_monthly_task.py; then
          TASK_END_TIME=$(date +%s)
          TASK_DURATION=$((TASK_END_TIME - TASK_START_TIME))
          echo "❌ Monthly task execution failed"
          echo "Checking for any generated files:"
          find . -name "*.json" -o -name "*.db" | head -10
          
          # 发送失败通知
          if [ -n "$FEISHU_WEBHOOK_URL" ]; then
            echo "📤 发送任务失败通知..."
            curl -X POST -H "Content-Type: application/json" -d '{
              "msg_type": "interactive",
              "card": {
                "elements": [
                  {
                    "tag": "div",
                    "text": {
                      "content": "**📦 仓库**: Bilibili_Flow\n**🔄 工作流**: Monthly Historical Data Processing\n**📅 时间**: '$(date '+%Y-%m-%d %H:%M:%S')'\n**🏷️ 运行**: #${{ github.run_number }}\n**🌿 分支**: ${{ github.ref_name }}\n**⏱️ 执行时长**: '$(($TASK_DURATION / 60))' 分钟\n**❌ 状态**: 任务执行失败\n\n**错误信息**: 请查看工作流日志获取详细信息",
                      "tag": "lark_md"
                    }
                  }
                ],
                "header": {
                  "title": {
                    "content": "❌ 月度数据处理失败",
                    "tag": "plain_text"
                  },
                  "template": "red"
                }
              }
            }' "$FEISHU_WEBHOOK_URL" || echo "飞书通知发送失败"
          fi
          exit 1
        else
          TASK_END_TIME=$(date +%s)
          TASK_DURATION=$((TASK_END_TIME - TASK_START_TIME))
          echo "✅ Monthly task execution completed"
          echo "Checking generated data files:"
          find data/ -type f | head -10
          
          # 统计生成的文件
          JSON_COUNT=$(find data/ -name "*.json" | wc -l)
          DB_COUNT=$(find data/ -name "*.db" | wc -l)
          TOTAL_SIZE=$(du -sh data/ 2>/dev/null | cut -f1 || echo "未知")
          
          # 发送成功通知
          if [ -n "$FEISHU_WEBHOOK_URL" ]; then
            echo "📤 发送任务成功通知..."
            curl -X POST -H "Content-Type: application/json" -d '{
              "msg_type": "interactive",
              "card": {
                "elements": [
                  {
                    "tag": "div",
                    "text": {
                      "content": "**📦 仓库**: Bilibili_Flow\n**🔄 工作流**: Monthly Historical Data Processing\n**📅 时间**: '$(date '+%Y-%m-%d %H:%M:%S')'\n**🏷️ 运行**: #${{ github.run_number }}\n**🌿 分支**: ${{ github.ref_name }}\n**⏱️ 执行时长**: '$(($TASK_DURATION / 60))' 分钟\n**✅ 状态**: 任务执行成功\n\n**📊 数据统计**\n📄 JSON文件: '$JSON_COUNT' 个\n🗄️ 数据库文件: '$DB_COUNT' 个\n💾 数据总大小: '$TOTAL_SIZE'\n\n**🎯 任务完成**: 月度历史数据处理完成，数据已保存到仓库",
                      "tag": "lark_md"
                    }
                  }
                ],
                "header": {
                  "title": {
                    "content": "✅ 月度数据处理成功",
                    "tag": "plain_text"
                  },
                  "template": "green"
                }
              }
            }' "$FEISHU_WEBHOOK_URL" || echo "飞书通知发送失败"
          fi
        fi
        
    - name: Commit and push data
      run: |
        echo "🔍 Checking for changes in data directory"
        
        # Force add all data subdirectories to ensure new files are tracked
        if [ -d "data/" ]; then
          git add -A data/
          
          # Check if there are changes to commit
          if ! git diff --staged --quiet; then
            git commit -m "📊 Weekly historical data processing - $(date +'%Y-%m-%d %H:%M:%S')"
            git push
            echo "✅ Data committed and pushed successfully"
          else
            echo "ℹ️ No changes to commit in data directory"
          fi
        else
          echo "⚠️ Data directory doesn't exist - no data to commit"
        fi
        
    - name: Generate data summary report
      if: always()
      run: |
        echo "## Weekly Data Processing Report - $(date +'%Y-%m-%d')" > weekly_report.md
        echo "### Processing Summary" >> weekly_report.md
        
        # Check if data directory exists and has content
        if [ -d "data/" ] && [ "$(find data/ -name "*.json" | wc -l)" -gt 0 ]; then
          echo "- ✅ Data directory found with JSON files" >> weekly_report.md
          echo "- ✅ Historical data processing appears successful" >> weekly_report.md
          echo "- ✅ 28-day segmentation applied" >> weekly_report.md
          echo "- ✅ Database baseline established" >> weekly_report.md
        else
          echo "- ❌ Historical data processing encountered errors" >> weekly_report.md
          echo "- ⚠️ Data directory empty or missing JSON files" >> weekly_report.md
          echo "- ⚠️ Check workflow logs for details" >> weekly_report.md
        fi
        
        echo "" >> weekly_report.md
        echo "### File Structure" >> weekly_report.md
        if [ -d "data/" ]; then
          echo "Data directory contents:" >> weekly_report.md
          find data/ -type f -name "*.json" | head -20 | while read file; do
            size=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null || echo "unknown")
            echo "- $file (${size} bytes)" >> weekly_report.md
          done
          
          if [ "$(find data/ -name "*.json" | wc -l)" -eq 0 ]; then
            echo "- No JSON files found in data directory" >> weekly_report.md
          fi
        else
          echo "- Data directory not found" >> weekly_report.md
        fi
        
        echo "" >> weekly_report.md
        echo "### System Information" >> weekly_report.md
        echo "- Workflow: weekly-historical-processing" >> weekly_report.md
        echo "- Runner: ubuntu-latest" >> weekly_report.md
        echo "- Python version: 3.11" >> weekly_report.md
        echo "- Timestamp: $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> weekly_report.md
        
        # Create placeholder files if they don't exist
        touch cookie_management_report.json
        touch cookie_cleanup_report.json  
        touch failed_cookies_report.json
        touch cleanup_cookies.sh
        
        # Add basic content to empty files with timestamps
        timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
        [ ! -s cookie_management_report.json ] && echo "{\"status\": \"no_report_generated\", \"timestamp\": \"$timestamp\", \"workflow\": \"weekly-historical\"}" > cookie_management_report.json
        [ ! -s cookie_cleanup_report.json ] && echo "{\"status\": \"no_cleanup_performed\", \"timestamp\": \"$timestamp\", \"workflow\": \"weekly-historical\"}" > cookie_cleanup_report.json
        [ ! -s failed_cookies_report.json ] && echo "{\"failed_cookies\": [], \"timestamp\": \"$timestamp\", \"workflow\": \"weekly-historical\"}" > failed_cookies_report.json
        [ ! -s cleanup_cookies.sh ] && echo "#!/bin/bash\necho \"No cleanup script generated for weekly-historical workflow - $timestamp\"" > cleanup_cookies.sh
        
    - name: Upload report and cookie reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: weekly-report-${{ github.run_number }}
        path: |
          weekly_report.md
          debug_report.json
          cookie_management_report.json
          cookie_cleanup_report.json
          failed_cookies_report.json
          cleanup_cookies.sh
        retention-days: 30
